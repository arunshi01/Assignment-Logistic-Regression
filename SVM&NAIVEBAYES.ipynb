{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhkXfLmgukInTf09vSPUbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunshi01/Assignment-Logistic-Regression/blob/main/SVM%26NAIVEBAYES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "A Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification (and sometimes regression) tasks. It’s particularly effective in high-dimensional spaces and works well when there is a clear margin of separation between classes.\n",
        "\n",
        "\n",
        "How SVM Works – Step-by-Step\n",
        "\n",
        "I)Plot the data in a feature space.\n",
        "\n",
        "II)Find the hyperplane that separates the classes.\n",
        "The goal is to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        "III)Use support vectors (the points on the edge of the margin) to define this hyperplane.\n",
        "\n",
        "IV)When new data comes in, SVM determines on which side of the hyperplane it lies — and classifies it accordingly."
      ],
      "metadata": {
        "id": "Ym1wfYB7U0TK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) : Explain the difference between Hard Margin and Soft Margin SVM\n",
        "\n",
        "Hard Margin SVM\n",
        "\n",
        "Used when data is perfectly linearly separable — i.e., you can draw a straight line (or hyperplane) that separates the two classes with no misclassifications.\n",
        "\n",
        "Idea:\n",
        "\n",
        "-The SVM tries to find the hyperplane that maximizes the margin between the two classes.\n",
        "\n",
        "-No data points are allowed to be on the wrong side of the hyperplane or even within the margin.\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "Used when data is not perfectly separable — which is common in real-world datasets.\n",
        "\n",
        "Idea:\n",
        "\n",
        "-Allows some misclassifications or margin violations to achieve a better overall boundary.\n",
        "\n",
        "-Introduces a penalty term for points that fall inside the margin or on the wrong side of the hyperplane."
      ],
      "metadata": {
        "id": "tH6x8japVgnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "The Kernel Trick allows an SVM to handle non-linearly separable data by implicitly mapping it into a higher-dimensional space — without actually computing that transformation.\n",
        "\n",
        "This means SVM can find a linear boundary in that higher-dimensional space, which corresponds to a nonlinear boundary in the original space.\n",
        "Example-RBF (Gaussian) Kernel for curved/nonlinear boundaries.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "Imagine classifying whether a patient has a disease based on two features (say, blood sugar and BMI).\n",
        "If the relationship between healthy and sick patients is nonlinear (e.g., curved boundary), an RBF kernel can help the SVM learn that complex pattern effectively."
      ],
      "metadata": {
        "id": "G-sG-2EqWEvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "The Naïve Bayes Classifier is based on Bayes’ Theorem, which describes how to update the probability of a hypothesis based on new evidence.\n",
        "\n",
        "It’s used to predict the class of a data point given its features by calculating posterior probabilities.\n",
        "\n",
        "It’s called “naïve” because it assumes that all features are independent of each other given the class label.\n"
      ],
      "metadata": {
        "id": "_5fNTvuhWv5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Gaussian Naïve Bayes (GNB)\n",
        "When to Use:\n",
        "-When your features are continuous (numerical values).\n",
        "-Common in datasets with measurements like height, weight, age, temperature, etc.\n",
        "\n",
        "Assumption:\n",
        "\n",
        "Each feature follows a normal (Gaussian) distribution within each class.\n",
        "\n",
        "Multinomial Naïve Bayes (MNB)\n",
        "When to Use:\n",
        "-When your features are counts or frequencies.\n",
        "-Most common for text classification, e.g., spam detection, sentiment analysis.\n",
        "\n",
        "Assumption:\n",
        "\n",
        "Features represent discrete counts (e.g., number of times a word appears)\n",
        "\n",
        "Bernoulli Naïve Bayes (BNB)\n",
        "When to Use:\n",
        "-When your features are binary (0 or 1).\n",
        "-Indicates presence or absence of a feature, not how many times it appears.\n",
        "\n",
        "Assumption:\n",
        "\n",
        "Each feature is a binary variable (1 = present, 0 = absent).\n"
      ],
      "metadata": {
        "id": "jFeJpZC2XNhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) : Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "G1-gjBsaYKYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1️⃣ Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Labels\n",
        "\n",
        "# 2️⃣ Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Train an SVM Classifier with a linear kernel\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4️⃣ Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5️⃣ Calculate and print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6️⃣ Print the support vectors\n",
        "print(\"\\nSupport Vectors:\\n\", model.support_vectors_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBdcYFhvYN51",
        "outputId": "9009d5c2-7cee-46b0-cadb-3740927ea6a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n"
      ],
      "metadata": {
        "id": "tdufm60bYiBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1️⃣ Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data      # Features\n",
        "y = breast_cancer.target    # Labels\n",
        "\n",
        "# 2️⃣ Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Train a Gaussian Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4️⃣ Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5️⃣ Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY-ZW9SPYmoL",
        "outputId": "d568cb2e-1ff4-465c-ffec-d6bb0c8679da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8) Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n"
      ],
      "metadata": {
        "id": "I3ZBvC8xYuJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1️⃣ Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data       # Features\n",
        "y = wine.target     # Labels\n",
        "\n",
        "# 2️⃣ Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Define the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# 4️⃣ Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf']   # using RBF kernel for non-linear mapping\n",
        "}\n",
        "\n",
        "# 5️⃣ Apply GridSearchCV to find the best parameters\n",
        "grid = GridSearchCV(svm, param_grid, refit=True, cv=5, verbose=0)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6️⃣ Print the best hyperparameters\n",
        "print(\"Best Hyperparameters found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "# 7️⃣ Evaluate the model on test data\n",
        "y_pred = grid.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "e7Z3Dk6UY00p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n"
      ],
      "metadata": {
        "id": "xL6KrzB4ZvU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# 1️⃣ Load the 20 Newsgroups dataset\n",
        "categories = ['sci.space', 'rec.sport.baseball']  # using 2 categories for binary classification\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "\n",
        "# 2️⃣ Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3️⃣ Convert text data into TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# 4️⃣ Train a Multinomial Naïve Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# 5️⃣ Predict probabilities for the test set\n",
        "y_probs = model.predict_proba(X_test_tfidf)[:, 1]  # probability for positive class\n",
        "\n",
        "# 6️⃣ Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edtvm5qxZyda",
        "outputId": "40faa46c-fd96-4483-850d-281825e59365"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.995617387759262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10) : Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "Problem Context\n",
        "\n",
        "We need to build an automated system to classify emails as Spam or Not Spam — a binary text classification problem.\n",
        "Challenges include:\n",
        "\n",
        "-Unstructured text data\n",
        "-Class imbalance (more “Not Spam” than “Spam”)\n",
        "-Missing or incomplete content\n",
        "\n",
        "1)Data Preprocessing\n",
        "a) Handling Missing Data\n",
        "-Some emails might have missing subject lines or bodies.\n",
        "Approaches:\n",
        "-Drop emails with completely missing text.\n",
        "-Impute missing parts with placeholders (e.g., “no_subject”, “no_body”) to preserve record count.\n",
        "b) Text Cleaning\n",
        "-Convert all text to lowercase.\n",
        "-Remove HTML tags, punctuation, stopwords, and extra spaces.\n",
        "-Perform tokenization and optionally lemmatization (reducing words to their base form).\n",
        "c) Text Vectorization\n",
        "-To convert raw text into numerical features:\n",
        "-Use TF-IDF (Term Frequency–Inverse Document Frequency), which:\n",
        "-Captures how important a word is relative to all documents.\n",
        "-Reduces the impact of common words (“the”, “and”, etc.).\n",
        "-You can also try CountVectorizer for simpler models or word embeddings (Word2Vec, BERT) for deep learning approaches.\n",
        "\n",
        "2) Model Choice: SVM vs. Naïve Bayes\n",
        "\n",
        "3) Handling Class Imbalance\n",
        "\n",
        "Since spam is usually less frequent than legitimate emails:\n",
        "\n",
        "Techniques:\n",
        "\n",
        "i)Resampling:\n",
        "-Oversampling (SMOTE) – synthetically generate spam samples.\n",
        "-Undersampling – reduce non-spam samples.\n",
        "\n",
        "ii)Class Weights:\n",
        "-In SVM or logistic regression, set class_weight='balanced' to give more importance to the minority class.\n",
        "\n",
        "iii)Threshold Tuning:\n",
        "-Adjust classification probability threshold (e.g., label as spam if P(spam) > 0.4 instead of 0.5).\n",
        "\n",
        "4) Model Evaluation\n",
        "-Precision: How many predicted spam emails are actually spam (avoid false alarms).\n",
        "-Recall: How many actual spam emails were correctly detected (avoid missing spam).\n",
        "-F1-Score: Balances precision and recall.\n",
        "-ROC-AUC Score: Measures how well the model separates the two classes.\n",
        "-Confusion Matrix: Shows the trade-off between spam and not-spam predictions.\n",
        "\n",
        "5)Business Impact\n",
        "\n",
        "Implementing this spam detection system provides clear business value."
      ],
      "metadata": {
        "id": "msBnDsVGZ86p"
      }
    }
  ]
}